{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import toolpy as tp\n",
    "from toolpy.tool.tool import TextLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from toolpy.integrations import groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_interface = groq.GroqInterface(model=groq.GroqModel.LLAMA3_70B, n_retry=5)\n",
    "\n",
    "registry = tp.llm.LLMRegistry()\n",
    "registry.registry(model_name=\"llama3-70b\", interface=groq_interface, default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerator(tp.BasicTool):\n",
    "    _description = \"Generates questions from a list of memories\"\n",
    "    _input_description = {\"memories\": \"a list of memory descriptions\"}\n",
    "\n",
    "    _system_message = '''You are a question generator that outputs in JSON.\n",
    "The JSON must use the schema: {'questions': ['str', 'str', 'str']}. \n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''\n",
    "Given the following list of memories:\n",
    "{memories}\n",
    "\n",
    "What are 3 most salient high-level questions we can answer about the subjects in these statements?'''\n",
    "\n",
    "    _return_description = {'questions': 'a list of three high-level questions'}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(description=self._description,\n",
    "                         input_description=self._input_description,\n",
    "                         prompt_template=self._base_prompt,\n",
    "                         return_description=self._return_description,\n",
    "                         system_message=self._system_message,\n",
    "                         model_name=model_name,\n",
    "                         json_mode=True)\n",
    "        \n",
    "    def _execute(self, query: Dict[str, str], context: str) -> Tuple[Dict[str, TextLike], Dict[str, str]]:\n",
    "        result, return_description = super()._execute(query, context)\n",
    "        \n",
    "        questions = result[\"questions\"]\n",
    "        \n",
    "        return {\"questions\": questions}, return_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import deque\n",
    "from typing import List\n",
    "\n",
    "import cst_python as cst\n",
    "\n",
    "# from .question_generator_tool import QuestionGenerator\n",
    "\n",
    "class QuestionGeneratorCodelet(cst.Codelet):\n",
    "    def __init__(self, model_name: str | None = None, \n",
    "                 memories_name: str | None = None, \n",
    "                 generated_questions_name: str | None = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if memories_name is None:\n",
    "            memories_name = \"Memories\"\n",
    "        if generated_questions_name is None:\n",
    "            generated_questions_name = \"GeneratedQuestions\"\n",
    "\n",
    "        self._memories_name = memories_name\n",
    "        self._generated_questions_name = generated_questions_name\n",
    "\n",
    "        self._question_generator = QuestionGenerator(model_name)\n",
    "\n",
    "    def access_memory_objects(self) -> None:\n",
    "        self._memories_mo = self.get_input(name=self._memories_name)\n",
    "        self._generated_questions_mo = self.get_output(name=self._generated_questions_name)\n",
    "\n",
    "    def calculate_activation(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def proc(self) -> None:\n",
    "        memories: List[str] = self._memories_mo.get_info()\n",
    "        query = {\"memories\": \"\\n\".join(memories)}\n",
    "        result, _ = self._question_generator._execute(query, context=\"\")\n",
    "        questions = result[\"questions\"]\n",
    "\n",
    "        self._generated_questions_mo.set_info(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import toolpy as tp\n",
    "from toolpy.tool.tool import TextLike\n",
    "\n",
    "class InsightGenerator(tp.BasicTool):\n",
    "    _description = \"Generates high-level insights from a list of memory descriptions\"\n",
    "    _input_description = {\"statements\": \"a list of statements/memory descriptions about the agent\"}\n",
    "\n",
    "    _system_message = '''You are an insight generator that outputs in JSON.\n",
    "The JSON must use the schema: {'insights': ['str', 'str', 'str', 'str', 'str']}. \n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''\n",
    "Statements about the agent:\n",
    "{statements}\n",
    "\n",
    "What 5 high-level insights can you infer from the above statements? (example format: insight (because of 1, 5, 3))'''\n",
    "\n",
    "    _return_description = {'insights': 'a list of five high-level insights'}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(description=self._description,\n",
    "                         input_description=self._input_description,\n",
    "                         prompt_template=self._base_prompt,\n",
    "                         return_description=self._return_description,\n",
    "                         system_message=self._system_message,\n",
    "                         model_name=model_name,\n",
    "                         json_mode=True)\n",
    "\n",
    "    def _execute(self, query: Dict[str, str], context: str) -> Tuple[Dict[str, TextLike], Dict[str, str]]:\n",
    "        result, return_description = super()._execute(query, context)\n",
    "        \n",
    "        insights = result[\"insights\"]\n",
    "        \n",
    "        return {\"insights\": insights}, return_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import deque\n",
    "from typing import List\n",
    "\n",
    "import cst_python as cst\n",
    "\n",
    "# from .insight_generator_tool import InsightGenerator\n",
    "\n",
    "class InsightGeneratorCodelet(cst.Codelet):\n",
    "    def __init__(self, model_name: str | None = None, \n",
    "                 statements_name: str | None = None, \n",
    "                 generated_insights_name: str | None = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if statements_name is None:\n",
    "            statements_name = \"Statements\"\n",
    "        if generated_insights_name is None:\n",
    "            generated_insights_name = \"GeneratedInsights\"\n",
    "\n",
    "        self._statements_name = statements_name\n",
    "        self._generated_insights_name = generated_insights_name\n",
    "\n",
    "        self._insight_generator = InsightGenerator(model_name)\n",
    "\n",
    "    def access_memory_objects(self) -> None:\n",
    "        self._statements_mo = self.get_input(name=self._statements_name)\n",
    "        self._generated_insights_mo = self.get_output(name=self._generated_insights_name)\n",
    "\n",
    "    def calculate_activation(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def proc(self) -> None:\n",
    "        statements: List[str] = self._statements_mo.get_info()\n",
    "        query = {\"statements\": \"\\n\".join(statements)}\n",
    "        result, _ = self._insight_generator._execute(query, context=\"\")\n",
    "        insights = result[\"insights\"]\n",
    "\n",
    "        self._generated_insights_mo.set_info(insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "- What are the subject's academic achievements?\n",
      "- What are the subject's travel experiences?\n",
      "- What is the subject's occupation?\n",
      "\n",
      "Generated Insights:\n",
      "- Klaus Mueller is interested in social sciences and its applications.\n",
      "- Klaus Mueller values knowledge and personal growth.\n",
      "- Klaus Mueller is likely an extroverted and communicative person.\n",
      "- Klaus Mueller is probably an academic or student in the field of urban development.\n",
      "- Klaus Mueller is concerned about the community and its well-being.\n"
     ]
    }
   ],
   "source": [
    "# Define the sample input for memories and statements\n",
    "sample_memories = [\n",
    "    \"I graduated from college with honors.\",\n",
    "    \"I traveled to Japan and experienced the cherry blossom festival.\",\n",
    "    \"I started a new job in a field I'm passionate about.\"\n",
    "]\n",
    "\n",
    "sample_statements = [\n",
    "    \"Klaus Mueller is writing a research paper.\",\n",
    "    \"Klaus Mueller enjoys reading a book on gentrification.\",\n",
    "    \"Klaus Mueller is conversing with Ayesha Khan about exercising.\",\n",
    "    \"Klaus Mueller recently attended a conference on urban development.\",\n",
    "    \"Klaus Mueller often discusses social issues with friends.\"\n",
    "]\n",
    "\n",
    "# Initialize memory objects (assuming get_input and get_output are methods to interact with memory objects)\n",
    "class MockMemoryObject:\n",
    "    def __init__(self):\n",
    "        self._info = []\n",
    "\n",
    "    def get_info(self):\n",
    "        return self._info\n",
    "\n",
    "    def set_info(self, info):\n",
    "        self._info = info\n",
    "\n",
    "memories_mo = MockMemoryObject()\n",
    "memories_mo.set_info(sample_memories)\n",
    "\n",
    "generated_questions_mo = MockMemoryObject()\n",
    "\n",
    "statements_mo = MockMemoryObject()\n",
    "statements_mo.set_info(sample_statements)\n",
    "\n",
    "generated_insights_mo = MockMemoryObject()\n",
    "\n",
    "# Initialize the Codelet instances\n",
    "question_generator_codelet = QuestionGeneratorCodelet(model_name=\"gpt-4\")\n",
    "insight_generator_codelet = InsightGeneratorCodelet(model_name=\"gpt-4\")\n",
    "\n",
    "# Set the memory objects for the QuestionGeneratorCodelet\n",
    "question_generator_codelet._memories_mo = memories_mo\n",
    "question_generator_codelet._generated_questions_mo = generated_questions_mo\n",
    "\n",
    "# Execute the QuestionGeneratorCodelet\n",
    "question_generator_codelet.proc()\n",
    "\n",
    "# Retrieve the generated questions\n",
    "generated_questions = generated_questions_mo.get_info()\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for question in generated_questions:\n",
    "    print(f\"- {question}\")\n",
    "\n",
    "# Set the memory objects for the InsightGeneratorCodelet\n",
    "insight_generator_codelet._statements_mo = statements_mo\n",
    "insight_generator_codelet._generated_insights_mo = generated_insights_mo\n",
    "\n",
    "# Execute the InsightGeneratorCodelet\n",
    "insight_generator_codelet.proc()\n",
    "\n",
    "# Retrieve the generated insights\n",
    "generated_insights = generated_insights_mo.get_info()\n",
    "\n",
    "print(\"\\nGenerated Insights:\")\n",
    "for insight in generated_insights:\n",
    "    print(f\"- {insight}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
